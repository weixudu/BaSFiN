# BaSFiN: Bayesian Skill–Feature Interaction Network for NBA Match Prediction

Bayesian Skill–Feature Interaction Network (BaSFiN) is a Bayesian hierarchical model
designed to estimate cooperative and competitive player contributions in team-based sports.
This repository contains the official implementation accompanying a manuscript submitted to
the *Journal of Quantitative Analysis in Sports (JQAS)*.

The codebase is organized to support **full experimental reproducibility**.
In particular, this README documents **which scripts generate each figure and table**
reported in the paper.

---

## Installation

Install required Python dependencies:

    pip install -r code/requirements.txt

---

## Project Structure

The repository is organized as follows:

    BaSFiN_code/
    ├── code/                   # Main codebase for all experiments
    ├── data/                   # Feature and roster datasets (not publicly included)
    ├── .gitignore
    └── README.md

---

## Code Directory Overview

All experiments are executed with `code/` set as the working directory.

    cd code

The `code/` directory contains the following subfolders:

    code/
    ├── BaSFiN/                 # Main BaSFiN experiments (2013–2024)
    ├── BaSFiN_2009_2024/       # Extended-period experiments (2009–2024)
    ├── Baseline_Models/        # Baseline models (BT, NAC)
    ├── Processing/             # Data preprocessing and tensor construction
    ├── Search/                 # Hyperparameter search scripts
    ├── logs/                   # Training logs and experiment outputs
    └── model/                  # Model checkpoints and saved weights

---

## Data Availability

Due to data size and licensing constraints, raw NBA datasets are not included in this repository.

All scripts access the required data via file paths and assume that processed feature CSV
files and team roster files are located under a user-specified `data/` directory, following
the formats generated by the scripts in `Processing/`.


---

## Reproducibility Guide (Figures and Tables)

This section documents **which scripts reproduce each figure and table**
reported in the JQAS manuscript.

---

## Figures

### Figure 1 — AUC Convergence of the NeuralAC Baseline Model

Figure 1 reports the AUC convergence behavior of the original NeuralAC baseline model
under a two-stage training procedure.

- Figure 1(a) shows training, validation, and test AUC curves during STEP1 training.
- Figure 1(b) shows training and test AUC curves after the final retraining stage (STEP2).

These results are generated by executing the NeuralAC training scripts provided under:
  - `code/Baseline Models/NAC.py`
  - `code/BaSFiN/plot/plot_NAC.py`
The figure illustrates a clear overfitting pattern of the NeuralAC baseline: training AUC
monotonically increases with additional epochs, whereas validation and test AUC exhibit
early saturation and limited generalization. This behavior motivates the development and
evaluation of BaS and BaSFiN.

---

### Figure 2 — Comparison of Skill Distributions: BT vs. BaS

Figure 2 compares player skill-score distributions learned by the traditional
Bradley–Terry (BT) model and the proposed BaS model.

- Figure 2(a) plots games played versus historical win rate, with color indicating
  estimated player skill under the BT model.
- Figure 2(b) shows the same visualization using skill estimates obtained from the BaS model.

Both panels are generated using player-level statistics derived from the 2013–2024 dataset.

The results are reproduced by executing the corresponding analysis scripts under:
  
  -`code/BaSFiN/train_basfin_noInter.py`

---

### Figure 3 — Validation AUC Surface over KL Weight and Learning Rate

- Dataset: 2013–2024

Figure 3 reports the validation AUC surface of the BaSFiN model as a function of the
KL divergence weight (λ_KL) and the learning rate (η).

- Hyperparameter search is conducted using:

  -`code/Search/train_BaS_random.py`

- The validation AUC surface is visualized using:

  -`code/BaSFiN/plot/plot_BaS_KL_LR.py`

Each point on the surface represents the mean validation AUC obtained under the
corresponding hyperparameter configuration. The figure is used to illustrate the
systematic and smooth dependence of model performance on key variational
optimization parameters, rather than to compare predictive performance across models.

---

### Figure 4 — Convergence Trend of Player Uncertainty (σ)

- Dataset: 2013–2024

Figure 4 illustrates the convergence trajectories of posterior uncertainty parameters (σ)
for selected players during BaSFiN training.

- Training results are generated by executing:

  - `code/BaSFiN/train_BaS.py`

- The uncertainty trajectories are visualized using:

  - `code/BaSFiN/plot/sigma_plot.py`

Each curve corresponds to a representative player selected for illustration, with relatively
small posterior uncertainty. Although local fluctuations are observed due to stochastic
gradient updates, all trajectories exhibit a clear downward trend before stabilizing at
nonzero values. This behavior reflects the effect of KL regularization and indicates that
the model successfully captures uncertainty patterns associated with sample size and
performance variability.

---

### Figure 5 — Validation AUC Comparison Across Training Strategies and Model Architectures

- Dataset: 2013–2024

Figure 5 compares validation AUC across different BaSFiN model variants as a function
of the probabilistic latent dimension.

The following configurations are evaluated:
- Frozen probabilistic backbone with MLP classifier
- End-to-end training with MLP classifier
- Frozen probabilistic backbone with linear classifier
- End-to-end training with linear classifier

- Model training and evaluation are conducted using:

  - `code/Search/train_basfin_random_noInter.py`  
    (frozen-backbone configurations)

  - `code/Search/train_basfin_random_noInter_nofreeze.py`  
    (end-to-end training configurations)

- The resulting validation AUC curves are visualized using:

  - `code/BaSFiN/plot/basfin_inter_compare.py`

The figure is used to assess the sensitivity of validation performance to training
strategy and classifier architecture under a controlled experimental setting,
rather than to introduce additional model comparisons.

---

## Tables

### Table 1 — Summary Statistics of NBA Matches

- Dataset: 2013–2024
- Script location:

      code/Processing/

- Description:
  - Constructs descriptive statistics reported in the data section of the paper.

---

### Table 2 — Main Predictive Performance Results

- Dataset: 2013–2024
- Models:
  - BaSFiN
  - Bradley–Terry (BT)
  - Neural Additive Composition (NAC)

- Scripts:

      code/BaSFiN/train_basfin_noInter.py
      code/Baseline_Models/BT/
      code/Baseline_Models/NAC/

---

### Table 3 — Extended-Period Results (2009–2024)

- Dataset: 2009–2024
- Script:

      code/BaSFiN_2009_2024/train_basfin_noInter.py

---

## Execution Workflow

### Step 1 — Set Working Directory

    cd BaSFiN_code/code/

---

### Step 2 — Pretraining

Pretrain cooperative and competitive feature interaction modules:

    pretrain_fim.py

Pretraining stabilizes subsequent joint training of the full BaSFiN model.

---

### Step 3 — Main Training (2013–2024)

    train_basfin_noInter.py

Key argument:

    force_no_freeze = False

This controls whether pretrained modules are frozen during joint training.

---

### Step 4 — Extended-Period Training (2009–2024)

Switch to:

    code/BaSFiN_2009_2024/

Then run:

    train_basfin_noInter.py

The workflow is identical to the 2013–2024 version, with extended historical data.

---

### Step 5 — Hyperparameter Search

Hyperparameter tuning is conducted using scripts under:

    code/Search/

These experiments are performed on the 2013–2024 dataset.

---

## Notes

- The `model/` directory typically requires no modification.
- The `logs/` directory may grow large during experimentation.
- Pretraining is strongly recommended before full BaSFiN training.
- All models are evaluated using identical data splits and metrics to ensure fair comparison.

---

## Citation

If you use this code, please cite the accompanying manuscript submitted to the
*Journal of Quantitative Analysis in Sports (JQAS)*.
