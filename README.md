# BaSFiN: Bayesian Skill–Feature Interaction Network for NBA Match Prediction

Bayesian Skill–Feature Interaction Network (BaSFiN) is a Bayesian hierarchical model
designed to estimate cooperative and competitive player contributions in team-based sports.
This repository contains the official implementation accompanying a manuscript submitted to
the *Journal of Quantitative Analysis in Sports (JQAS)*.

The codebase is organized to support **full experimental reproducibility**.
In particular, this README documents **which scripts generate each figure and table**
reported in the paper.

---

## Installation

Install required Python dependencies:

    pip install -r code/requirements.txt

---

## Project Structure

The repository is organized as follows:

    BaSFiN
    ├── code/                   # Main codebase for all experiments
    ├── data/                   # Feature and roster datasets (not publicly included)
    ├── .gitignore
    ├── requirements.txt
    └── README.md

---

## Code Directory Overview

All experiments are executed with `code/` set as the working directory.

    cd code

The `code/` directory contains the following subfolders:

    code/
    ├── BaSFiN/                 # Main BaSFiN experiments (2013–2024)
    ├── BaSFiN_2009_2024/       # Extended-period experiments (2009–2024)
    ├── Baseline_Models/        # Baseline models (BT, NAC)
    ├── Processing/             # Data preprocessing and tensor construction
    ├── Search/                 # Hyperparameter search scripts
    ├── logs/                   # Training logs and experiment outputs
    └── model/                  # Model checkpoints and saved weights

---

## Data Availability

Due to data size and licensing constraints, raw NBA datasets are not included in this repository.

All scripts access the required data via file paths and assume that processed feature CSV
files and team roster files are located under a user-specified `data/` directory, following
the formats generated by the scripts in `Processing/`.


---

## Reproducibility Guide (Figures and Tables)

This section documents **which scripts reproduce each figure and table**
reported in the JQAS manuscript.

---

## Figures

### Figure 1 — AUC Convergence of the NeuralAC Baseline Model

Figure 1 reports the AUC convergence behavior of the original NeuralAC baseline model
under a two-stage training procedure.

- Figure 1(a) shows training, validation, and test AUC curves during STEP1 training.
- Figure 1(b) shows training and test AUC curves after the final retraining stage (STEP2).

These results are generated by executing the NeuralAC training scripts provided under:
  - `code/Baseline Models/NAC.py`
  - `code/BaSFiN/plot/plot_NAC.py`

The figure illustrates a clear overfitting pattern of the NeuralAC baseline: training AUC
monotonically increases with additional epochs, whereas validation and test AUC exhibit
early saturation and limited generalization. This behavior motivates the development and
evaluation of BaS and BaSFiN.

---

### Figure 2 — Comparison of Skill Distributions: BT vs. BaS

Figure 2 compares player skill-score distributions learned by the traditional
Bradley–Terry (BT) model and the proposed BaS model.

- Figure 2(a) plots games played versus historical win rate, with color indicating
  estimated player skill under the BT model.
- Figure 2(b) shows the same visualization using skill estimates obtained from the BaS model.

Both panels are generated using player-level statistics derived from the 2013–2024 dataset.

The results are reproduced by executing the corresponding analysis scripts under:
  
  -`code/BaSFiN/train_basfin_noInter.py`

---

### Figure 3 — Validation AUC Surface over KL Weight and Learning Rate

- Dataset: 2013–2024

Figure 3 reports the validation AUC surface of the BaSFiN model as a function of the
KL divergence weight (λ_KL) and the learning rate (η).

- Hyperparameter search is conducted using:

  -`code/Search/train_BaS_random.py`

- The validation AUC surface is visualized using:

  -`code/BaSFiN/plot/plot_BaS_KL_LR.py`

Each point on the surface represents the mean validation AUC obtained under the
corresponding hyperparameter configuration. The figure is used to illustrate the
systematic and smooth dependence of model performance on key variational
optimization parameters, rather than to compare predictive performance across models.

---

### Figure 4 — Convergence Trend of Player Uncertainty (σ)

- Dataset: 2013–2024

Figure 4 illustrates the convergence trajectories of posterior uncertainty parameters (σ)
for selected players during BaSFiN training.

- Training results are generated by executing:

  - `code/BaSFiN/train_BaS.py`

- The uncertainty trajectories are visualized using:

  - `code/BaSFiN/plot/sigma_plot.py`

Each curve corresponds to a representative player selected for illustration, with relatively
small posterior uncertainty. Although local fluctuations are observed due to stochastic
gradient updates, all trajectories exhibit a clear downward trend before stabilizing at
nonzero values. This behavior reflects the effect of KL regularization and indicates that
the model successfully captures uncertainty patterns associated with sample size and
performance variability.

---

### Figure 5 — Validation AUC Comparison Across Training Strategies and Model Architectures

- Dataset: 2013–2024

Figure 5 compares validation AUC across different BaSFiN model variants as a function
of the probabilistic latent dimension.

The following configurations are evaluated:
- Frozen probabilistic backbone with MLP classifier
- End-to-end training with MLP classifier
- Frozen probabilistic backbone with linear classifier
- End-to-end training with linear classifier

- Model training and evaluation are conducted using:

  - `code/Search/train_basfin_random_noInter.py`  
    (frozen-backbone configurations)

  - `code/Search/train_basfin_random_noInter_nofreeze.py`  
    (end-to-end training configurations)

- The resulting validation AUC curves are visualized using:

  - `code/BaSFiN/plot/basfin_inter_compare.py`

The figure is used to assess the sensitivity of validation performance to training
strategy and classifier architecture under a controlled experimental setting,
rather than to introduce additional model comparisons.

---

- Dataset: 2013–2024

- Script location:

   -`code/Processing/ema_emse_2024.py`

- Description:
  - Applies simple exponential smoothing (SES) independently to each player-level box-score feature.
  - Computes root mean squared error (RMSE) between the original time series and the fitted values.
  - Aggregates RMSE across players to assess the temporal stability of each feature.
  - Features with mean RMSE ≥ 10 are removed prior to downstream modeling.
  - This screening step results in 33 retained features used as dynamic inputs in subsequent experiments.
---

### Table 4 — Test-set Performance Comparison Between BaS and BT

- Dataset: 2013–2024

- Experimental setting:
  - BaS and BT models are evaluated under identical data splits and training conditions.
  - Two-stage evaluation protocol is adopted (Stage 1 and Stage 2 test sets).

- Script location:

   -`code/BaSFiN/train_BaS.py`
  
   -`code/Baseline_Models/BT/`
  
   -`code/BaSFiN/statistic/BaS_vs_Bt.py`

- Metrics reported:
  - Area Under the ROC Curve (AUC)
  - Classification Accuracy
  - Log Loss

- Description:
  - Reports predictive performance of the proposed BaS model against the Bradley–Terry (BT) baseline.
  - BaS consistently outperforms BT across all metrics in both Stage 1 and Stage 2 test sets.
  - After final retraining (Stage 2), BaS achieves a substantial AUC improvement and a notable reduction in log loss, indicating superior predictive reliability.
---

### Table 5 — Statistical Significance Tests (BaS vs. BT)

Table 5 reports statistical significance tests used to assess whether the performance
improvements of the BaS model over the BT baseline are statistically meaningful.

- Evaluation setting: Final retraining stage (test set)
- Compared models: BaS vs. BT

The following tests are conducted:
- **DeLong test** for comparing AUC values
- **McNemar test** for comparing classification accuracy

Both tests strongly reject the null hypothesis of equal predictive performance, with
extremely small p-values, indicating that the observed improvements of the BaS model
are not attributable to random variation.

- Results are generated by scripts under:

   -`code/BaSFiN/train_BaS.py`
  
   -`code/Baseline_Models/BT/`
  
   -`code/BaSFiN/statistic/BaS_vs_Bt.py`

---

### Table 6 — Impact of Dynamic (EMA-Based) Features

Table 6 evaluates the effect of incorporating dynamic, EMA-based temporal features
on predictive performance for both the competition and cooperation modules.

- Evaluation settings:
  - Stage 1 test set
  - Stage 2 final test set
- Compared designs:
  - With dynamic (EMA-based) features
  - Without dynamic features (static historical averages)

Performance is reported using AUC and accuracy (ACC). Results show that including
dynamic features consistently improves predictive performance across both modules
and both evaluation stages. This indicates that short-term temporal dynamics provide
additional behavioral information beyond static historical statistics.

- Results are generated using the following scripts:

   -`code/BaSFiN/train_bc_ema.py`
  
   -`code/BaSFiN/train_bc_noema.py`
  
   -`code/BaSFiN/train_cofim_noema.py`
  
   -`code/BaSFiN/train_cofim.py`
  
   -`code/BaSFiN/statistic/bc_compare.py/`
  
   -`code/BaSFiN/statistic/co_compare.py`

---

### Table 8 — Hyperparameter Sensitivity Analysis for FiN Modules

Table 8 reports the top five hyperparameter configurations identified through a
randomized hyperparameter search for both the cooperation and competition FiN
modules.

- Search objective:
  - Validation AUC
- Evaluation protocol:
  - Validation AUC averaged over five random seeds per configuration
- Search scope:
  - Attention dimension (`d_a`)
  - Intermediate dimension (`d_mid`)
  - MLP dimension (`d_mlp`)
  - Dropout rate
  - Attention usage flag (`NeedAtt`)

For each module, the table lists the five configurations achieving the highest
validation AUC, along with the corresponding parameter settings. The narrow AUC
ranges observed across top configurations indicate that both cooperation and
competition modules are robust to hyperparameter variation within the explored
search space.

- Results are generated using randomized search scripts under:

   -`code/Search/co_random.py`
  
   -`code/Search/bc_random.py`
  
   -`code/statistic/co_stat.py`
  
   -`code/statistic/bc_stat.py`
---

### Tables 9–10 — Interpretable Pairwise Interaction Scores (Case Studies)

Tables 9 and 10 present qualitative case studies illustrating the interpretability of
the learned pairwise interaction scores produced by the BaSFiN framework.

- **Table 9** focuses on *LeBron James*.  
  - Implemented by setting `FOCUS_PID = 1` in both `train_bc.py` and `train_cofim.py`.
- **Table 10** focuses on *Stephen Curry*.  
  - Implemented by setting `FOCUS_PID = 355` in both `train_bc.py` and `train_cofim.py`.

For each focal player, three types of interaction scores are reported:
- **Offensive advantage** against specific defenders
- **Defensive suppression** against opposing offensive players
- **Team-level cooperation (synergy)** scores with teammates

These scores are derived from the learned **competition** and **cooperation** modules
and are intended for **interpretability analysis rather than predictive evaluation**.
Observed interaction patterns are consistent with historical matchups and established
basketball domain knowledge, such as size mismatches, rim protection effects, and role
complementarity among teammates.

Together, Tables 9 and 10 demonstrate that the learned interaction representations
capture meaningful and human-interpretable basketball relationships, supporting the
validity of the proposed modeling framework beyond aggregate performance metrics.


#### Logging and Aggregation of Pairwise Interaction Statistics

After completing training with the competition and cooperation modules, we further
log summary statistics of the learned pairwise interaction scores to support systematic
interpretability analysis.

Specifically, after running:

 -`code/BaSFiN/train_bc.py`
 
 -`code/BaSFiN/train_cofim.py`

the final trained model weights (using combined Train + Validation data) are used to
compute aggregate interaction statistics, including:

- Global average top-3 and bottom-3 interaction pairs across all players
- Player-specific (PID-focused) average top-3 and bottom-3 interaction pairs

When a focal player is specified via `FOCUS_PID`, the following statistics are computed:
- Avg-Top-3 interaction pairs involving the focal player
- Avg-Bot-3 interaction pairs involving the focal player
- Corresponding average interaction scores and occurrence counts

All statistics are automatically written to plain-text log files (`.txt`) under the
configured output directory. For reproducibility and organization, the logs are then
grouped by player ID and stored in the corresponding directories:

- Competition-module logs:

   -`code/BaSFiN/statistic/pid_bc.py`

- Cooperation-module logs:

   -`code/BaSFiN/statistic/pid_cofim.py`

Each focal player has a dedicated log file under these directories, enabling systematic
player-level inspection of learned interaction patterns.

This logging and aggregation pipeline provides the empirical basis for the case-study
results reported in Tables 9–10, ensuring that the reported interaction patterns are
derived from reproducible, automatically generated summaries of the final trained
model parameters.


---
### Table 11 — Long-Term Robustness Across Rolling-Year Windows

Table 11 evaluates the long-term robustness and temporal generalization ability of
BaSFiN by comparing predictive AUC across multiple rolling-window year ranges.

- Evaluated year windows:
  - 2009–2020
  - 2010–2021
  - 2011–2022
  - 2012–2023
  - 2013–2024

- Compared models:
  - Bradley–Terry (BT)
  - Neural Additive Composition (NAC)
  - BaSFiN (proposed)

Performance is reported as mean AUC ± standard deviation across repeated runs.
Results show that BaSFiN consistently achieves the highest predictive AUC and the
lowest variance across all temporal windows, indicating superior robustness to
seasonal shifts, roster turnover, and evolving player dynamics.

This experiment demonstrates that the proposed BaSFiN framework generalizes
effectively over long time horizons and maintains stable performance under
substantial temporal distribution shifts.

- Results are generated using rolling-window experiments under:

   -`code/Baseline_Models/2009_2024_BT.py`
   -`code/Baseline_Models/2009_2024_NAC.py`
   -`code/BaSFiN_2009_2024/2009_2024_BT.py`
   -`code/BaSFiN_2009_2024/pretrain_fim.py`
   -`code/BaSFiN_2009_2024/train_basfin_nointer.py`
---
## Execution Workflow

### Step 1 — Set Working Directory

    cd BaSFiN_code/code/

---

### Step 2 — Pretraining

Pretrain cooperative and competitive feature interaction modules:

    pretrain_fim.py

Pretraining stabilizes subsequent joint training of the full BaSFiN model.

---

### Step 3 — Main Training (2013–2024)

    train_basfin_noInter.py

Key argument:

    force_no_freeze = False

This controls whether pretrained modules are frozen during joint training.

---

### Step 4 — Extended-Period Training (2009–2024)

Switch to:

    code/BaSFiN_2009_2024/

Then run:

    train_basfin_noInter.py

The workflow is identical to the 2013–2024 version, with extended historical data.

---

### Step 5 — Hyperparameter Search

Hyperparameter tuning is conducted using scripts under:

    code/Search/

These experiments are performed on the 2013–2024 dataset.

---

## Notes

- The `model/` directory typically requires no modification.
- The `logs/` directory may grow large during experimentation.
- Pretraining is strongly recommended before full BaSFiN training.
- All models are evaluated using identical data splits and metrics to ensure fair comparison.

---

## Citation

If you use this code, please cite the accompanying manuscript submitted to the
*Journal of Quantitative Analysis in Sports (JQAS)*.
